{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assignment 3 - Deep Learning with RNNs**\n",
    "\n",
    "### **Tianhui Zhang (001566190)**\n",
    "\n",
    "### **Abstract**\n",
    "\n",
    "In this assignment, I chose RNN (recurrent neural network) part. Because I have used CNN before but don't know much about RNN.\n",
    "\n",
    "A recurrent neural network is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
    "\n",
    "The following models are from [Hugging Face](https://huggingface.co/models). \n",
    "\n",
    "For the mainly used library Transformers, its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. Before the training or testing, pipeline of Transformers groups together three steps: preprocessing with a tokenizer (to convert the text inputs into numbers that the model can make sense of), passing the inputs through the model, and postprocessing the output (to convert logits, the raw, unnormalized scores outputted by the last layer of the model, to probabilities).\n",
    "\n",
    "I followed the official documents, tried standard example and then test with customized data to evaluate its performance in each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Questions:**\n",
    "\n",
    "**1. Fill-Mask (10 Points)**    Run a Fill-Mask language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"hello i'm a fashion model.\",\n",
       "  'score': 0.10731089115142822,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion'},\n",
       " {'sequence': \"hello i'm a role model.\",\n",
       "  'score': 0.08774477988481522,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role'},\n",
       " {'sequence': \"hello i'm a new model.\",\n",
       "  'score': 0.05338386073708534,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new'},\n",
       " {'sequence': \"hello i'm a super model.\",\n",
       "  'score': 0.04667213186621666,\n",
       "  'token': 3565,\n",
       "  'token_str': 'super'},\n",
       " {'sequence': \"hello i'm a fine model.\",\n",
       "  'score': 0.027095839381217957,\n",
       "  'token': 2986,\n",
       "  'token_str': 'fine'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Hugging face \n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': '\" now \" he said, you ’ d better meet my wife. she is the one who really started this school.',\n",
       "  'score': 0.47722530364990234,\n",
       "  'token': 2318,\n",
       "  'token_str': 'started'},\n",
       " {'sequence': '\" now \" he said, you ’ d better meet my wife. she is the one who really runs this school.',\n",
       "  'score': 0.16045495867729187,\n",
       "  'token': 3216,\n",
       "  'token_str': 'runs'},\n",
       " {'sequence': '\" now \" he said, you ’ d better meet my wife. she is the one who really founded this school.',\n",
       "  'score': 0.03581678867340088,\n",
       "  'token': 2631,\n",
       "  'token_str': 'founded'},\n",
       " {'sequence': '\" now \" he said, you ’ d better meet my wife. she is the one who really created this school.',\n",
       "  'score': 0.0325615257024765,\n",
       "  'token': 2580,\n",
       "  'token_str': 'created'},\n",
       " {'sequence': '\" now \" he said, you ’ d better meet my wife. she is the one who really built this school.',\n",
       "  'score': 0.0309604499489069,\n",
       "  'token': 2328,\n",
       "  'token_str': 'built'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of Hugging face \n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"\\\"Now\\\" he said, you’d better meet my wife. She is the one who really [MASK] this school.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a basic version of BERT (Bidirectional Encoder Representations from Transformers) model. According to the paper by Jacob Devlin et al., There are two steps in the framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.\n",
    "\n",
    "I chose a cloze question of China's College entrance examination as a test sentence. The correct answer is \"runs\", but I think \"starts\" is also correct if it's without context. The model are highly accurate in this sentence. But there is one of the weaknesses of the model, which has no way of making judgments based on long or hidden context information. So if you describe a sentence that's too neutral, it will be a big deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Question Answering (10 Points)**  Run a Question Answering language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:316: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  fw_args = {k: torch.tensor(v, device=self.device) for (k, v) in fw_args.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n",
      "Answer: 'SQuAD dataset', score: 0.5152, start: 148, end: 161\n"
     ]
    }
   ],
   "source": [
    "# Example of Hugging face\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. \n",
    "An example of a question answering dataset is the SQuAD dataset, which is entirely based on that \n",
    "task. If you would like to fine-tune a model on a SQuAD task, you may leverage the \n",
    "examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "\n",
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'to understand the challenges faced by older drivers', score: 0.1853, start: 136, end: 187\n",
      "Answer: 'preserving their \n",
      "independence', score: 0.3683, start: 810, end: 840\n",
      "Answer: 'continue driving into later life', score: 0.3947, start: 553, end: 585\n"
     ]
    }
   ],
   "source": [
    "# test of Hugging face\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "The Intelligent Transport team at Newcastle University have turned an electric car into a mobile laboratory named \n",
    "“DriveLAB” in order to understand the challenges faced by older drivers and to discover where the key stress points are.\n",
    "Research shows that giving up driving is one of the key reasons for a fall in health and well-being among older people, \n",
    "leading to them becoming more isolated and inactive.\n",
    "Led by Professor Phil Blythe, the Newcastle team are developing in-vehicle technologies for older drivers which they hope \n",
    "could help them to continue driving into later life.\n",
    "These include custom-made navigation tools, night vision systems and intelligent speed adaptations. Phil Blythe explains: \n",
    "“For many older people, particularly those living alone or in the country, driving is important for preserving their \n",
    "independence, giving them the freedom to get out and about without having to rely on others.”\n",
    "\"But we all have to accept that as we get older our reactions slow down and this often results in people avoiding any \n",
    "potentially challenging driving conditions and losing confidence in their driving skills. The result is that people stop \n",
    "driving before they really need to.\"\n",
    "Dr Amy Guo, the leading researcher on the older driver study, explains, \"The DriveLAB is helping us to understand what \n",
    "the key points and difficulties are for older drivers and how we might use technology to address these problems.\n",
    "\"For example, most of us would expect older drivers always go slower than everyone else but surprisingly, we found that \n",
    "in 30mph zones they struggled to keep at a constant speed and so were more likely to break the speed limit and be at risk \n",
    "of getting fined. We’re looking at the benefits of systems which control their speed as a way of preventing that.\n",
    "\"We hope that our work will help with technological solutions to ensure that older drivers stay safer behind the wheel.\"\n",
    "\"\"\"\n",
    "\n",
    "result = question_answerer(question=\"What is the purpose of the Drivel AB?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = question_answerer(question=\"Why is driving important for older people according to Phil Blythe?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = question_answerer(question=\"What do researchers hope to do for older drivers?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive Question Answering is the task of extracting an answer from a text given a question. According to the official document of Hugging Face, the process is shown below:\n",
    "\n",
    ">1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    ">2. Define a text and a few questions.\n",
    ">3. Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks.\n",
    ">4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    ">5. Compute the softmax of the result to get probabilities over the tokens.\n",
    ">6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    ">7. Print the results.\n",
    "\n",
    "Using pipelines to do question answering extract an answer from a text given a question. It leverages a fine-tuned model on SQuAD. This returns an answer extracted from the text, a confidence score, alongside “start” and “end” values, which are the positions of the extracted answer in the text.\n",
    "\n",
    "\n",
    "I chose an article of China's College entrance examination as the input, tested it with the above model and tried to avoid the way of asking questions directly in the article. According to the test results, the model has excellent ability to answer the questions. Although the questions are deformed, it can still find the appropriate answer (although the third question has some general answers). However, it still has some drawbacks. The first is that the answer must have appeared in the original text. For example, it can't understand the feelings or positions the author wants to express behind the article. Secondly, answers will be truncated, so questions that require longer answers may not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Summarization (10 Points)**    Run a Summarization language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\transformers\\generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos has been married 10 times, sometimes within two weeks of each other. At one time, she was married to eight men at once, prosecutors say. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes \n",
    "only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \n",
    "\"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" \n",
    "referring to her false statements on the 2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, \n",
    "who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly \n",
    "sneaking into the New York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. \n",
    "In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to \n",
    "four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly \n",
    "after the marriages. Any divorces happened only after such filings were approved. It was unclear whether any of the men \n",
    "will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department \n",
    "of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, \n",
    "Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint \n",
    "Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Newcastle University is developing in-vehicle technologies for older drivers. These include navigation tools, night vision systems and intelligent speed adaptations. Research shows that giving up driving is one of the key reasons for a fall in health and well-being among older people.'}]\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\"\n",
    "The Intelligent Transport team at Newcastle University have turned an electric car into a mobile laboratory named \n",
    "“DriveLAB” in order to understand the challenges faced by older drivers and to discover where the key stress points are.\n",
    "Research shows that giving up driving is one of the key reasons for a fall in health and well-being among older people, \n",
    "leading to them becoming more isolated and inactive.\n",
    "Led by Professor Phil Blythe, the Newcastle team are developing in-vehicle technologies for older drivers which they hope \n",
    "could help them to continue driving into later life.\n",
    "These include custom-made navigation tools, night vision systems and intelligent speed adaptations. Phil Blythe explains: \n",
    "“For many older people, particularly those living alone or in the country, driving is important for preserving their \n",
    "independence, giving them the freedom to get out and about without having to rely on others.”\n",
    "\"But we all have to accept that as we get older our reactions slow down and this often results in people avoiding any \n",
    "potentially challenging driving conditions and losing confidence in their driving skills. The result is that people stop \n",
    "driving before they really need to.\"\n",
    "Dr Amy Guo, the leading researcher on the older driver study, explains, \"The DriveLAB is helping us to understand what \n",
    "the key points and difficulties are for older drivers and how we might use technology to address these problems.\n",
    "\"For example, most of us would expect older drivers always go slower than everyone else but surprisingly, we found that \n",
    "in 30mph zones they struggled to keep at a constant speed and so were more likely to break the speed limit and be at risk \n",
    "of getting fined. We’re looking at the benefits of systems which control their speed as a way of preventing that.\n",
    "\"We hope that our work will help with technological solutions to ensure that older drivers stay safer behind the wheel.\"\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Alex: I feel it awful, I prefer steaks. Bill: I like eating'}]\n"
     ]
    }
   ],
   "source": [
    "# test 2\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\"\n",
    "Bill: I like eating pizza, how about you?\n",
    "Alex: I feel it awful, I prefer steaks.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=20, min_length=10, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary generation task, the input sequence is the document we want to summarize and the output sequence is a summary of the document. Bart-large-cnn model is the basic model that is fine-tuned with CNN/Daily Mail Abstractive Summarization Task. The Seq2Seq architecture can be used directly for summary tasks without any new operations, and pre-training tasks are well suited for downstream tasks.\n",
    "\n",
    "In terms of methods, text abstracts can be divided into two categories: Extractive and Abstractive. The former abstracts sentences from the original text directly, while the latter generates abstracts word by word. In comparison, the extraction method can not summarize the content of the original text roundly because of its inherent characteristics. The generative method is more flexible, but it is prone to make errors. For example, the third test shows the output content contrary to the original text. \n",
    "\n",
    "Before putting into use, it is very important to make sure that the summary of the model summary is consistent with the meaning of the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Text Classification (10 Points)** Run a Text Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'NEGATIVE', 'score': 0.0007330990047194064}, {'label': 'POSITIVE', 'score': 0.9992668628692627}]]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\",\n",
    "model='distilbert-base-uncased-finetuned-sst-2-english', \n",
    "return_all_scores=True)\n",
    "prediction = classifier(\n",
    "    \"I love using transformers. The best part is wide range of support and its easy to use\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'NEGATIVE', 'score': 0.9927522540092468}, {'label': 'POSITIVE', 'score': 0.007247743662446737}]]\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",model='distilbert-base-uncased-finetuned-sst-2-english', \n",
    "    return_all_scores=True)\n",
    "prediction = classifier(\"Nobody doesn't like this restaurant.\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. According to the introduction, it has better accuracy. Bert, which is pre-trained on a large number of data sets, usually only needs fine tuning to run well on many NLP tasks in this experiment. For emotional classification, it needs to add one or two linear layers at the last layer, output category probability, set cross entropy as loss, and train on a small number of data sets. That is input the same input to encoder and decoder. Finally, the last hidden node of decoder is input to classification layer (full connection layer) to obtain the final classification result.\n",
    "\n",
    "This method can judge whether the overall emotion of the input is positive or negative by grasping the emotional tendency of keywords. The advantage is that it can be used for the analysis of long film reviews or restaurant reviews to simplify the problem, but it cannot recognize indirect expressions of emotion (like \"Nobody doesn't like this restaurant.\" It will be negative.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Text Generation (10 Points)** Run a Text Generation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 8, but ``max_length`` is set to 0.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, I've got some more features to add and we'll get it done. In fact, I think there's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, I've got some more features to add and we'll get it done. In fact, I think there's a lot of potential here. In that respect, I wish it would be possible. If there's potential, what's the next step? And I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, I've got some more features to add and we'll get it done. In fact, I think there's a lot of potential here. In that respect, I wish it would be possible. If there's potential, what's the next step? And I've kind of got these ideas, as it turns out, and the next step is going forward, but we're at this very early stage, and\n",
      "Hello, I'm a language model, I've got some more features to add and we'll get it done. In fact, I think there's a lot of potential here. In that respect, I wish it would be possible. If there's potential, what's the next step? And I've kind of got these ideas, as it turns out, and the next step is going forward, but we're at this very early stage, and it's kind of very early, and we'll see what's happened in the next round.\"\n",
      "\n",
      "Hilbert didn't reveal that his work\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "input = \"Hello, I'm a language model,\"\n",
    "for i in range(0,5):\n",
    "    g = generator(input, max_length=30*i, num_return_sequences=2)\n",
    "    input = g[0]['generated_text']\n",
    "    print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, I\\'ve got some more features to add and we\\'ll get it done. In fact, I think there\\'s a lot of potential here. In that respect, I wish it would be possible. If there\\'s potential, what\\'s the next step? And I\\'ve kind of got these ideas, as it turns out, and the next step is going forward, but we\\'re at this very early stage, and it\\'s kind of very early, and we\\'ll see what\\'s happened in the next round.\"\\n\\nHilbert didn\\'t reveal that his work here was just the beginning of the work that would go into his application. But he acknowledged that work was still to come.\\n\\n\"We\\'ll try to do some more,\" said the Michigan native in a prepared statement. \"I don\\'t really know everything yet. But I\\'m sure we\\'ll get it out that we\\'re pretty sure it has some things that could be done that are very early on.\"'},\n",
       " {'generated_text': 'Hello, I\\'m a language model, and a language has to respect the language.\"\\n\\nKrebs said he\\'s always made sure people understood his own thinking, and he\\'s never done anything wrong. When I asked him why, he told me he was interested in the conversation, but he\\'s never gotten around to making them. I asked Krebs to write a second answer that would answer almost any question he could:\\n\\n\"If you\\'re interested in something then check out Reddit. The most popular Reddit forum in the world…you might not believe it when you look up your question and it gives you a glimpse into your mindset, your personal view and the world of the subject.\"\\n\\n\"If you\\'re interested in something then check out Reddit. The most popular Reddit forum in the world…you might not believe it when you look up your question and it gives you a glimpse into your mindset, your personal view and the world of the subject,\" Krebs told me. \"You might find out the truth about your assumptions through reading something like Gizmodo…because they don\\'t always have that good of a background. Maybe they\\'re a self-professed feminist or what have you.\"\\n\\n\\nThat\\'s a lot more information than I\\'m able to provide, but what I do know is, Krebs was incredibly passionate about philosophy and philosophy of language–for example, most popular philosophy of this day is about the nature of language; philosophy of language refers to how'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 2\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=300, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I used to be a positive man, until I realised that I'm actually a negative man... it's just like I think what I'm doing is wrong.\\n\\nBENNETT : No man has ever done anything like that. If you did, it would have been really obvious. And I've had people say 'what are you thinking all of a sudden?' because you're going, 'what do you mean? Who am I writing about? Why are you writing this?'\\n\\nSIMON : I've got no idea what you're talking about.\\n\\nBENNETT : I understand, if you do it a few times over then it may come back again. It's just you have to recognise that you're out there doing that one little thing that's important to you in life, not that you're making anything up, or that you don't believe what you're talking about.\\n\\nSIMON : So do you?\\n\\nBENNETT : I do. I can hardly think of a time where that's something else I would ever write about, which is never going to happen. It would be funny to say that.\\n\\nSIMON : How are you now?\\n\\nBENNETT : I've been here for six years and I've done it all this time, what have I got? A million stories and no one's ever asked me about it. It's all new and I hope to start doing it.\\n\"},\n",
       " {'generated_text': 'I used to be a positive man, until somebody was pushing down my trousers. I would say \\'no\\' to all that and say \\'shut up\\'…but then it\\'s not a man anymore.\" Then he found his best mate, David, in hospital with a broken rib, and became his second love. Her boyfriend, who is now 49, also suffers from depression. They are both known to get very different responses when they talk about love. \"Everybody wants someone to love them, especially when it means being married, and they don\\'t give a s**t when they\\'re depressed and you have to sit through all that and you\\'re having to find somebody with the ability to do that,\" says Rizmo. With the couple staying together for just over a year, we sat down with him at the BBC to chat about his life, his love of drugs, and the future of Rizmo. He\\'s joined by a couple of friends as well as his former boss, Steve Smith, founder and editor at Rizmo magazine. \"I was like, \\'I like it. It\\'s cool. I\\'m always getting stuff. I think it\\'s an honour. I do it quite a bit\\'.\"\\n\\n\\nAnd then came the news of his ex-girlfriend. But not for long. In November 2013, he moved back in to our hometown of Bristol after a 12-year romance. He had already had a couple of partners, but they were all just too different.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 3\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"I used to be a positive man, until\", max_length=300, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text generation task, I used GPT-2 model developed by OpenAI. This model was trained on (and evaluated against) WebText, a dataset consisting of the text contents of 45 million links posted by users of the ‘Reddit’ social network. WebText is made of data derived from outbound links from Reddit and does not consist of data taken directly from Reddit itself. More precisely, it was trained to guess the next word in sentences. The source data is input into encoder, and the target text to be generated is input into decoder for auto-regression generation.\n",
    "\n",
    "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token only uses the inputs from to.\n",
    "\n",
    "In Tests 1 and 2, I found that there was not much difference in the results of producing 300 lengths of text at a time and gradually producing 30 lengths of text at a time, which might provide less variety if used as a writing aid. But the generation is smooth and error-free in non-story paragraphs.\n",
    "\n",
    "However, with story input, the generated result may be a little wired (like what generated in test 3). As the OpenAI team themselves point out in their model card:\n",
    ">Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Text2Text Generation (10 Points)** Run a Text2Text language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',\n",
    "  max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, \n",
    "  num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The test of your knowledge is your ability to convey it.',\n",
       " 'The ability to convey your knowledge is the ultimate test of your knowledge.',\n",
       " 'The ability to convey your knowledge is the most important test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test of it.',\n",
       " 'The test of your knowledge is your ability to communicate it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge to another is the ultimate test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge is the most important test of your knowledge.',\n",
       " 'The test of your knowledge is how well you can convey it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "context = \"The ultimate test of your knowledge is your capacity to convey it to another.\"\n",
    "get_response(context,num_return_sequences,num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The chart shows that revenues more than doubled in 2009.',\n",
       " 'In 2009, revenues more than doubled from the year before.',\n",
       " 'In 2009, revenues more than doubled from 2008.',\n",
       " 'Revenues more than doubled from 2008 as shown in the chart above.',\n",
       " 'In 2009, revenues more than doubled.',\n",
       " 'Revenues more than doubled from 2008 to 2009.',\n",
       " 'The chart shows that revenues doubled from 2008 to 2009.',\n",
       " 'In 2009, revenues more than doubled from the previous year.',\n",
       " 'The chart shows that revenues doubled in 2009.',\n",
       " 'The chart shows that in 2009, revenues more than doubled.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "context = \"As the chart above shows, revenues in 2009 more than doubled from 2008.\"\n",
    "get_response(context,num_return_sequences,num_beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text2TextGeneration is the pipeline for text to text generation using seq2seq models. It is a single pipeline for all kinds of NLP tasks like Question answering, sentiment classification, question generation, translation, paraphrasing, summarization, etc. More like a rich version supporting more training tasks.\n",
    "\n",
    "I use the model pegasus_paraphrase. It is used to generate different representations of input statements. It can be used to enrich the sentence patterns used in writing. Results appear to be generated by extraction and composition, and produces nothing wrong. Looks enough for its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Token Classification (10 Points)** Run a Token Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Wolfgang',\n",
       "  'score': 0.9990139603614807,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 4,\n",
       "  'start': 11,\n",
       "  'end': 19},\n",
       " {'word': 'Berlin',\n",
       "  'score': 0.9996449947357178,\n",
       "  'entity': 'B-LOC',\n",
       "  'index': 9,\n",
       "  'start': 34,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# example\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "nlp(\"My name is Wolfgang and I live in Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Ke',\n",
       "  'score': 0.9995712041854858,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'word': '##ira',\n",
       "  'score': 0.9950459599494934,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 2,\n",
       "  'start': 2,\n",
       "  'end': 5},\n",
       " {'word': 'Knight',\n",
       "  'score': 0.9996829628944397,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 3,\n",
       "  'start': 6,\n",
       "  'end': 12},\n",
       " {'word': '##ley',\n",
       "  'score': 0.9962692260742188,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 4,\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'word': 'France',\n",
       "  'score': 0.9996482729911804,\n",
       "  'entity': 'B-LOC',\n",
       "  'index': 16,\n",
       "  'start': 57,\n",
       "  'end': 63}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "nlp(\"Keira Knightley got married in a low-key in the south of France on Saturday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
    "\n",
    "Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\n",
    "\n",
    "For Token classification tasks, the complete input is input into encoder and decoder, and all the hidden nodes in the last layer of decoder are represented as the model of each Token, and then the representation of each Token is classified, and finally the result output is obtained.\n",
    "\n",
    "The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n",
    "\n",
    "It shows high accuracy with the example input. But when test with other input, it will divide a whole name into 2 parts or highlight wrong words. According to the model card:\n",
    "\n",
    ">This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Translation (10 Points)** Run a Translation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eu gosto de comer arroz.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n",
    "\n",
    "# example\n",
    "enpt_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "enpt_pipeline(\"translate English to Portuguese: I like to eat rice.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eu gosto de comer arroz.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "enpt_pipeline(\"translate English to Portuguese: I like to eat rice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. Google explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. According to the modell card of t5:\n",
    "\n",
    ">Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n",
    "\n",
    "This model brings an implementation of T5 for translation in EN-PT tasks using a modest hardware setup. It proposes some changes in tokenizator and post-processing that improves the result and used a Portuguese pretrained model for the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Zero-Shot Classification (10 Points)** Run a Zero-Shot language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# example\n",
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to('cpu'))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'space & cosmos': 5.2, 'scientific discovery': 71.2, 'microbiology': 23.7}\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "premise = \"\"\"A new model offers an explanation for how the Galilean satellites formed around the solar system’s \n",
    "largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries \n",
    "when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his \n",
    "contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 \n",
    "degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back \n",
    "holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical \n",
    "astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in \n",
    "a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a \n",
    "planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form \n",
    "around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the \n",
    "solar system’s formation became trapped around these massive worlds, taking shape one at a time into the \n",
    "potentially habitable moons we know today.\"\"\"\n",
    "\n",
    "input = tokenizer(premise, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to('cpu'))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"space & cosmos\", \"scientific discovery\", \"microbiology\", \"robots\", \"archeology\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the model DeBERTa-v3-base-mnli-fever-anli. It can determine whether two paragraphs are related or whether one paragraph is related to several groups of keywords. From the tests, its accuracy was pretty high.\n",
    "\n",
    "According to the paper by Pengcheng He ec al., they improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand (NLU) and natural langauge generation (NLG) downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Sentence Similarity (10 Points)** Run a Sentence Similarity language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<A>All plans come with unlimited private models and datasets.', '<A>AutoNLP seamlessly integrated with the Hugging Face ecosystem.', '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link.']\n",
      "<A>All plans come with unlimited private models and datasets.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# example\n",
    "question = \"<Q>How many models can I host on HuggingFace?\"\n",
    "answers = [\n",
    "  \"<A>All plans come with unlimited private models and datasets.\",\n",
    "  \"<A>AutoNLP seamlessly integrated with the Hugging Face ecosystem.\",\n",
    "  \"<A>Based on how much training data and model variants are created, we send you a compute cost and payment link.\"\n",
    "]\n",
    "model = SentenceTransformer('clips/mfaq')\n",
    "q_embedding, *a_embeddings = model.encode([question] + answers)\n",
    "print(answers)\n",
    "best_answer_idx = sorted(enumerate(a_embeddings), key=lambda x: q_embedding.dot(x[1]), reverse=True)[0][0]\n",
    "print(answers[best_answer_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<A>2 ounces of regular beer', '<A>Drinking an occasional glass of red wine is good for you.', '<A>the process of associating numbers with physical quantities and phenomena']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "question = \"<Q>what is a standard drink measurement\"\n",
    "answers = [\n",
    "  \"<A>2 ounces of regular beer\",\n",
    "  \"<A>Drinking an occasional glass of red wine is good for you.\",\n",
    "  \"<A>the process of associating numbers with physical quantities and phenomena\"\n",
    "]\n",
    "model = SentenceTransformer('clips/mfaq')\n",
    "q_embedding, *a_embeddings = model.encode([question] + answers)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mfaq model is a multilingual FAQ retrieval model trained on the MFAQ dataset, it ranks candidate answers according to a given question. Maxime De Bruyn ec al. collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics.\n",
    "\n",
    "I randomly choose a question from Google to test its performance. In order to increase the difficulty, I set two confusing answers: one has the keyword 'drink' in the question, and the other explains the keyword 'measurement' in the question. The correct answer is a phrase without any key words and the model passed the test.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
